{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scraper.guardian_dataset as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just to grab a bunch of random words that might fit into groups\n",
    "article = corpus.getArticle('https://www.theguardian.com/world/2020/sep/13/us-west-coast-choked-by-smoke-as-firefighters-tackle-deadly-wildfires')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-c373bc0f0062>:5: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629395347/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  vec_article.append(torch.from_numpy(word))\n"
     ]
    }
   ],
   "source": [
    "vec_article = list()\n",
    "for word in article:\n",
    "    try:\n",
    "        word = corpus.w2v[word]\n",
    "        vec_article.append(torch.from_numpy(word))\n",
    "    except KeyError:\n",
    "        pass\n",
    "article = vec_article\n",
    "del vec_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now comes time to rank each of the articles\n",
    "ranked = list()\n",
    "for word in article:\n",
    "    sort_article = sorted(article, key=lambda w : torch.cosine_similarity(w, word, dim=0), reverse=False)\n",
    "    ranked.append({\"word\":word, \"rankings\":sort_article})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets prove the algo first:\n",
    "test = ranked[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = [torch.cosine_similarity(w, test['word'], -1).item() for w in test['rankings']]\n",
    "distances = [i  for i in filter(lambda x: x!= 1, distances)]#Get rid of the same word, cause that messes up the scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking a quick look to prove that clear groupings exist\n",
    "%matplotlib qt\n",
    "#Kinda hard to see inline, need a larger output\n",
    "plt.figure()\n",
    "# plt.hlines(1, 1, max(distances)/2)\n",
    "plt.eventplot(distances, orientation=\"horizontal\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, so its clear that there are groupings. Now how do we actually separate them? It is very easy to do visually, however, it is very difficult to make a general solution that will always work in a computer, as we have to assume inperfection\n",
    "\n",
    "In order to break up the vectors, we can turn the above graph into a 2 dimensional graph, and use some hack calculus in order to figure out where the groups break off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making frequency graph 2-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Distance from word')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_distance = [0]\n",
    "#Forgive the bad code\n",
    "for d in range(1, len(distances)):\n",
    "    delta_distance.append(distances[d] - distances[d-1])\n",
    "plt.figure()\n",
    "plt.plot(delta_distance)\n",
    "plt.ylabel('Change in distance')\n",
    "plt.xlabel('Distance from word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the two above graphs right next to each other, one can see that there are obvious peaks in the 2-d graph between groups! \n",
    "Let's try to use these peaks to make groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = list()\n",
    "start_idx = 0\n",
    "running_avg = [0 for i in range(6)]\n",
    "for item in range(0, len(delta_distance)-2):\n",
    "    running_avg[0:5] = running_avg[1:6]\n",
    "    running_avg[5] = delta_distance[item+2] - delta_distance[item+1]\n",
    "    \n",
    "    if (sum(running_avg)/len(running_avg) < 0 and running_avg[5] < 0):\n",
    "        groups.append(test['rankings'][start_idx:item])\n",
    "        start_idx = item + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(group) for group in groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_len = sum(lengths)/len(lengths)\n",
    "avg_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thats a pretty good average size for groups, keep in mind that some words are going to be extraneous, and others are going to have more than average pairings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets see how well these pairings are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_dist_total = torch.zeros(1)\n",
    "counter_total = 0\n",
    "check = torch.zeros(300) == torch.ones(300)\n",
    "for group in groups:\n",
    "    for item in group:\n",
    "        for other in group:\n",
    "            \n",
    "            if not torch.all(torch.eq(item, other)):\n",
    "                sum_dist_total += torch.cosine_similarity(item, other, -1)\n",
    "                counter_total += 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2256])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_dist_total/counter_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So the pairings work out pretty well\n",
    "\n",
    "\n",
    "Now, lets look at the code to execute this on the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flashing back to above, remember that \"ranked\" is a list of dictionaries with the properties 'word' and 'ranking'\n",
    "#Note: for most uses of this algo, you will NOT need to do all of this, only some of the grouping stuff. Therefore it will not take as long to run\n",
    "\n",
    "output = list()\n",
    "for word in ranked:\n",
    "    distance = [torch.cosine_similarity(word['word'], i, -1) for i in word['rankings']]\n",
    "    delta_distance = [0]\n",
    "    for d in range(1, len(distances)):\n",
    "        delta_distance.append(distances[d] - distances[d-1])\n",
    "    groups = list()\n",
    "    start_idx = 0\n",
    "    running_avg = [0 for i in range(6)]\n",
    "    for item in range(0, len(delta_distance)-2):\n",
    "        running_avg[0:5] = running_avg[1:6]\n",
    "        running_avg[5] = delta_distance[item+2] - delta_distance[item+1]\n",
    "\n",
    "        if (sum(running_avg)/len(running_avg) < 0 and running_avg[5] < 0):\n",
    "            groups.append(test['rankings'][start_idx:item])\n",
    "            start_idx = item + 1\n",
    "    output.append(word.update({'groups':groups}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Grouping for text sumarization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_vec = torch.tensor([-1 for i in range(300)])\n",
    "article_ranked = sorted(article, key=lambda w : torch.cosine_similarity(w, min_vec, dim=0), reverse=False)\n",
    "distances = [torch.cosine_similarity(i, min_vec, dim=0) for i in article_ranked]\n",
    "delta_distance = [0]\n",
    "group_list = list()\n",
    "for d in range(1, len(distances)):\n",
    "    delta_distance.append(distances[d] - distances[d-1])\n",
    "    groups = list()\n",
    "    start_idx = 0\n",
    "    running_avg = [0 for i in range(6)]\n",
    "    for item in range(0, len(delta_distance)-2):\n",
    "        running_avg[0:5] = running_avg[1:6]\n",
    "        running_avg[5] = delta_distance[item+2] - delta_distance[item+1]\n",
    "\n",
    "\n",
    "        if (sum(running_avg)/len(running_avg) < 0 and running_avg[5] < 0):\n",
    "            groups.append(test['rankings'][start_idx:item])\n",
    "            start_idx = item + 1\n",
    "    group_list  += groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list of groups of related words.\n",
    "We can just now just take the words from each legit (len>1) group \n",
    "that have the highest similarity to all the other words in order to come up with a quick\n",
    "summary of what happened, which can then be fed into some type of anaysis or generator algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
